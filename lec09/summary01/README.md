# 9 Deep Learning Software

> [Lecture 9: Hardware and Software](https://www.youtube.com/watch?v=oXPX8GIOiU4&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=9)

> [slides - Lecture 12: Deep Learning Software](https://web.eecs.umich.edu/~justincj/slides/eecs498/WI2022/598_WI2022_lecture12.pdf)

ëŒ€í‘œì ì¸ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œê°€ ìˆë‹¤.

- Caffe(UC Berkely) $\rightarrow$ Caffe2(Facebook)

- Torch(NYU/Facebook) $\rightarrow$ PyTorch(Facebook)

- Theano(University of Montreal) $\rightarrow$ Tensorflow(Google)

- PaddlePaddle(Baidu)

- MXNet(Amazon)

- JAX(Google)

- CNTK(Microsoft)

ë³¸ ì •ë¦¬ì—ì„œëŠ” PyTorchì™€ TensorFlowì— ì´ˆì ì„ ë§ì¶˜ë‹¤.

---

## 9.1 The points of Deep Learning Frameworks

Deep Learning(DL) í”„ë ˆì„ì›Œí¬ëŠ” 3ê°€ì§€ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•´ì•¼ í•œë‹¤.

- í¸ë¦¬í•œ common layer, utilities ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

- gradients ê³„ì‚°ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤.

- GPUë‚˜ TPUë¥¼ í™œìš©í•´ íš¨ìœ¨ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

ì—¬ê¸°ì„œ gradients ê³„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì´ Computational graphë¥¼ êµ¬í˜„í•˜ì—¬ ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.

![computational graph](images/computational_graphs.png)

- forward pass

- backward pass

- update

---

## 9.2 PyTorch: Fundamental Concepts

> 1.10 ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±

PyTorchì—ì„œ ì œì¼ ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ì€ ì„¸ ê°€ì§€ë¡œ ìš”ì•…í•  ìˆ˜ ìˆë‹¤.

- Tensor

  numpy arrayì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, GPUì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

  > Tensor APIë§Œìœ¼ë¡œë„ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±, gradients ê³„ì‚°, parameter update ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

  > NCHW formatì„ ì‚¬ìš©í•œë‹¤. [(NCHWì™€ ê´€ë ¨í•´ì„œëŠ” ì •ë¦¬ ì°¸ì¡°)](https://github.com/erectbranch/TinyML_and_Efficient_DLC/tree/master/lec17)

- Autograd

  ìë™ìœ¼ë¡œ computational graphë¥¼ ë§Œë“¤ì–´ ì£¼ë©°, gradientsë¥¼ ê³„ì‚°í•´ì¤€ë‹¤.

- Module

  ì‹ ê²½ë§ì˜ layerë¥¼ ì˜ë¯¸í•œë‹¤. ëŒ€ì²´ë¡œ stateë‚˜ learnable weightsë¥¼ ê°€ì§„ë‹¤.

---

## 9.3 PyTorch: Tensor

> [PyTorch Docs: Tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)

Tensorë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ initializeí•˜ë©´ ëœë‹¤.

```Python
# 1. Direct from data
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)

# 2. From a NumPy array
np_array = np.array(data)
x_np = torch.from_numpy(np_array)

# 3. From another tensor
x_ones = torch.ones_like(x_data)     # retains the properties of x_data

x_rand = torch.rand_like(x_data)     # overrides the datatype of x_data
```

ìì£¼ ì‚¬ìš©í•˜ëŠ” tensor ë¬¸ë²• ì„¸ ê°€ì§€ë¥¼ ì‚´í´ë³´ì.

```Python
# 1. tuple í˜•íƒœì˜ shapeë¥¼ ì •ì˜í•œë‹¤.
shape = (2,3,)

# 2. Tensorë¥¼ ìƒì„±í•œë‹¤.
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)
```

> ì°¸ê³ ë¡œ PyTorchì—ì„œ TensorëŠ” autimatic differentiationì„ ìœ„í•´ optimizeëœë‹¤.

> PyTorchëŠ” arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling ë“± 100ê°œê°€ ë„˜ëŠ” tensor operationì„ ì§€ì›í•œë‹¤.

ë‹¤ìŒì€ Tensor operationë§Œìœ¼ë¡œ ìˆ˜ë™ìœ¼ë¡œ ëª¨ë“  í›ˆë ¨ ê³¼ì •ì„ êµ¬í˜„í•œ ì˜ˆì‹œ ì½”ë“œë‹¤.

- `torch.mm`: matrix multiplication

- ì˜µì…˜ì„ ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´(`.to`, `.device`), **tensorëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPUì—ì„œ ìƒì„±ëœë‹¤.**

  > CPUì™€ GPU ì‚¬ì´ì˜ data communicationì˜ ë¹„ìš©ì„ ê³ ë ¤í•˜ê¸° ë•Œë¬¸

```Python
import torch

# cpu: torch.device('cpu')
# gpu: torch.device('cuda:0')
device = torch.device('cuda:0')

# ì„ì˜ì˜ Tensorsë¥¼ ìƒì„±í•œë‹¤. 
# (x,y) ë°ì´í„°ì™€ weights(w1,w2)
N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in, device=device)    # GPU ì—°ì‚°ì„ ìœ„í•´ deviceì— ìƒì„±í•´ì•¼ í•œë‹¤.
y = torch.randn(N, D_out, device=device)
w1 = torch.randn(D_in, H, device=device)
w2 = torch.randn(H, D_out, device=device)

learning_rate = 1e-6
for t in range(500):
    # forward pass
    # predì™€ lossë¥¼ ê³„ì‚°í•œë‹¤.
    h = x.mm(w1)    # N x H
    h_relu = h.clamp(min=0)
    y_pred = h_relu.mm(w2)  # N x D_out
    loss = (y_pred - y).pow(2).sum()

    # backward pass
    # gradientë¥¼ ê³„ì‚°í•œë‹¤.
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)

    # update weights(gradient descent step)
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```

---

## 9.4 PyTorch: Autograd

ìœ„ ì˜ˆì‹œì—ì„œ Autogradë¥¼ ì´ìš©í•˜ë©´ ìë™ìœ¼ë¡œ gradientsë¥¼ ê³„ì‚°í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

```Python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10
# ë°ì´í„°ì¸ (x,y)ëŠ” gradient ê³„ì‚°ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)
# weightì¸ w1,w2ëŠ” gradient ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤.
# Autograd ì ìš©ì„ ìœ„í•´ì„œ 'requires_grad=True' ì˜µì…˜ì„ ì¶”ê°€í•œë‹¤.
w1 = torch.randn(D_in, H, requires_grad=True)
w2 = torch.randn(H, D_out, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    # forward pass
    # backpropagationë¥¼ ìœ„í•œ intermediate valuesë¥¼ êµ³ì´ ì €ì¥í•  í•„ìš”ê°€ ì—†ë‹¤.
    # (PyTorchê°€ ì•Œì•„ì„œ graphì— ê°’ì„ ë³´ê´€í•œë‹¤.)
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    loss = (y_pred - y).pow(2).sum()

    # backward pass
    # requires_grad=Trueë¡œ ì„¤ì •í•œ ëª¨ë“  Tensorsì—ì„œ
    # ì…ë ¥ì— ëŒ€í•œ gradient ê³„ì‚°ì„ ìˆ˜í–‰í•œë‹¤.
    loss.backward()

    # backwardê°€ ëë‚˜ë©´ gradientëŠ” w1.grad, w2.gradì— ì €ì¥ëœë‹¤.
    # ë™ì‹œì— graphëŠ” destroyëœë‹¤.
    # with torch.no_grad(): í•´ë‹¹ ì—°ì‚°ì—ì„œëŠ” graphë¥¼ buildí•˜ì§€ ì•ŠëŠ”ë‹¤.
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad

        # ëë‚œ ë’¤ gradientë¥¼ ë‹¤ì‹œ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ì•¼ ë²„ê·¸ê°€ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.
        w1.grad.zero_()
        w2.grad.zero_()
```

ì˜ˆì‹œ ì½”ë“œë¥¼ ë„ì‹í™”í•˜ì—¬ ì‚´í´ë³´ì.

- forward pass

  ![Autograd forward 1](images/Autograd_forward_1.png)

  ![Autograd forward 2](images/Autograd_forward_2.png)

- backward pass

  ![Autograd backward](images/Autograd_backward.png)

- update

  backwardê°€ ëë‚˜ì„œ graphê°€ destroyëœë‹¤.

  ![Autograd update](images/Autograd_end.png)

---

## 9.5 PyTorch: New Functions

ì´ë²ˆì—ëŠ” ê³„ì‚° ê³¼ì • ì¤‘ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì„œ ì‚¬ìš©í•´ ë³´ì. ë¨¼ì € ë‹¤ìŒê³¼ ê°™ì€ sigmoidë¥¼ ì •ì˜í•œë‹¤.

```Python
def sigmoid(x):
    return 1.0 / (1.0 + (-x).exp())
```

ì´ì œ ì •ì˜í•œ sigmoidë¥¼ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•´ ë³´ì.

```Python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)
w1 = torch.randn(D_in, H, requires_grad=True)
w2 = torch.randn(H, D_out, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    y_pred = sigmoid(x.mm(w1)).mm(w2)
    loss = (y_pred -y).pow(2).sum()

    loss.backward()
    if t % 50 == 0:
        print(t, loss.item())

    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        w1.grad.zero_()
        w2.grad.zero_()
```

graphëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![new function](images/function.png)

ë§Œì•½ subclassingì„ í†µí•´ ìƒˆë¡œìš´ autograd functionì„ ì •ì˜í•˜ê³  ì‹¶ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ forwardì™€ backwardë¥¼ í•¨ê»˜ êµ¬í˜„í•˜ë©´ ëœë‹¤.

```Python
class Sigmoid(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
      y = 1.0 / (1.0 + (-x).exp())
      ctx.save_for_backward(y)
      return y

    @staticmethod
    def backward(ctx, grad_y):
      y, = ctx.save_tensors
      grad_x = grad_y * y * (1.0 - y)
      return grad_x

def sigmoid(x):
  return Sigmoid.apply(x)
```

ì´ì œ ê¸°ì¡´ì˜ graphëŠ” ë‹¤ìŒê³¼ ê°™ì´ í•˜ë‚˜ì˜ functionìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆë‹¤.

![sigmoid](images/sigmoid.png)

í•˜ì§€ë§Œ ì´ë ‡ê²Œê¹Œì§€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê²½ìš°ëŠ” ë“œë¬¼ê³ , ëŒ€ë¶€ë¶„ íŒŒì´ì¬ì—ì„œ ê¸°ë³¸ìœ¼ë¡œ ì œê³µí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ë©´ ëœë‹¤.

```Python
def sigmoid(x):
  return 1.0 / (1.0 + (-x).exp())
```

---

## 9.6 PyTorch: nn

PyTorchì—ì„œëŠ” ì‰½ê²Œ ì‹ ê²½ë§ì„ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”, Object-oriented(ê°ì²´ ì§€í–¥) APIì¸ `nn` ëª¨ë“ˆì„ ì œê³µí•œë‹¤.

- ëª¨ë“  íŒ¨ëŸ¬ë¯¸í„°ëŠ” `parameters()` í˜¹ì€ `named_parameters()` ë©”ì„œë“œë¡œ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤.

  ![parameters](images/model_parameters.png)

```Python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Object-oriented API
# ë ˆì´ì–´ ê°ì²´ëŠ” ê°ìì˜ weight tensorsë¥¼ ê°–ëŠ”ë‹¤.
model = torch.nn.Sequential(
            torch.nn.Linear(D_in, H),
            torch.nn.ReLU(),
            torch.nn.Linear(H, D_out))

learning_rate = 1e-2
for t in range(500):
    # forward
    # loss functionsë¥¼ êµ¬í˜„í•  ë•Œ, torch.nn.functionalì„ ì‚¬ìš©í•˜ë©´ í¸ë¦¬í•˜ë‹¤.(ì´ì™¸ì—ë„ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©)
    y_pred = model(x)
    loss = torch.nn.functional.mse_loss(y_pred, y)

    # backward(requires_grad=True ëŒ€ìƒ gradient ê³„ì‚°)
    loss.backward()

    # ê° model parameterì— ëŒ€í•´ gradient stepì„ ìˆ˜í–‰í•œë‹¤.
    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad
    model.zero_grad()
```

---

## 9.7 PyTorch: optim

> [PyTorch Docs: optim](https://pytorch.org/docs/stable/optim.html)

ë˜í•œ ë‚˜ë§Œì˜ gradient descent ê·œì¹™ì„ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì´ ë²ˆê±°ë¡­ê¸° ë•Œë¬¸ì—, PyTorchì—ì„œëŠ” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `optim` ëª¨ë“ˆì„ ì œê³µí•œë‹¤.

> `optimizer.param_groups[0]["lr"]` ì½”ë“œë¥¼ í†µí•´ í˜„ì¬ learning rateë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

```Python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

model = torch.nn.Sequential(
                              torch.nn.Linear(D_in, H),
                              torch.nn.ReLU(),
                              torch.nn.Linear(H, D_out))

learning_rate = 1e-4
# optimizer(Adam ì‚¬ìš©)
# ì¸ìë¡œ ìµœì í™”í•  íŒ¨ëŸ¬ë¯¸í„° ì „ë‹¬(ì—¬ê¸°ì„œëŠ” model parameterë¥¼ ìµœì í™”)
# ë™ì‹œì— learning rateë„ í•„ìš”í•˜ë¯€ë¡œ ì „ë‹¬í•œë‹¤.
optimizer = torch.optim.Adam(model.paramters(),
                             lr=learning_rate)

for t in range(500):
  y_pred = model(x)
  loss = torch.nn.functional.mse_loss(y_pred, y)

  loss.backward()

  # backwardê°€ ëë‚˜ë©´(gradient ê³„ì‚°ì´ ëë‚˜ë©´) ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤.(step)
  # ë˜í•œ gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ë²„ê·¸ë¥¼ ë°©ì§€í•œë‹¤.
  optimizer.step()
  # print(optimizer.param_groups[0]["lr"])    # í˜„ì¬ learning rate í™•ì¸
  optimizer.zero_grad()
```

---

### 9.7.1 PyTorch: optim.lr_scheduler

> [PyTorch Learning rate scheduler ì •ë¦¬](https://sanghyu.tistory.com/113)

> [A Visual Guide to Learning Rate Schedulers in PyTorch](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)

PyTorchì—ì„œëŠ” ì´ epoch ìˆ˜ì— ë”°ë¼ì„œ learning rateë¥¼ ì¡°ì ˆí•˜ëŠ” `lr_scheduler` ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

![PyTorch lr_scheduler](images/torch_lr_scheduler.jpg)

ë‹¤ìŒì€ ExponetialLR `lr_scheduler`ì˜ êµ¬í˜„ ì˜ˆì‹œë‹¤.

- $\gamma$ : ë§¤ epochë§ˆë‹¤ learning rateì— ê³±í•´ì§€ëŠ” decay factor

$$ {lr}_{epoch} = \gamma \cdot {lr}_{epoch - 1} $$

```Python
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    # train
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    # validation stepì˜ ê²½ìš°, ì´ë•Œ validationì„ ìˆ˜í–‰

    scheduler.step()
```

---

## 9.8 PyTorch: Defining nn Modules

PyTorchì—ì„œ `nn.Module` ì‹ ê²½ë§ì˜ layerë¥¼ ì˜ë¯¸í•œë‹¤. weightsë‚˜ ë‹¤ë¥¸ modulesë¥¼ í¬í•¨í•˜ë©°, ì´ë¥¼ ìƒì†ë°›ì•„ ì‰½ê²Œ ì‹ ê²½ë§ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

- `__init__`ì—ì„œ NN layerë¥¼ ì´ˆê¸°í™”í•œë‹¤.

- backward ê³¼ì •ì€ Autogradë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ìˆ˜í–‰ë˜ë¯€ë¡œ ì •ì˜í•  í•„ìš”ê°€ ì—†ë‹¤.

```Python
import torch

class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    super(TwoLayerNet, self).__init__()
    self.linear1 = torch.nn.Linear(D_in, H)
    self.linear2 = torch.nn.Linear(H, D_out)

  def forward(self, x):
    h_relu = self.linear1(x).clamp(min=0)
    y_pred = self.linear2(h_relu)
    return y_pred

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

model = TwoLayerNet(D_in, H, D_out)

optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for t in range(500):
  y_pred = model(x)
  loss = torch.nn.functional.mse_loss(y_pred, y)

  loss.backward()
  optimizer.step()
  optimizer.zero_grad()
```

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Defining Modules &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒê³¼ ê°™ì€ ì‹ ê²½ë§ graphë¥¼ ëª¨ë“ˆ í•˜ë‚˜ë¥¼ ì‘ì„±í•˜ì—¬ ì´ì–´ë¶™ì´ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ë¼.

![module ex](images/module_ex.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

```Python
import torch

class ParallelBlock(torch.nn.Module):
  def __init__(self, D_in, D_out):
    super(ParallelBlock, self).__init__()
    self.linear1 = torch.nn.Linear(D_in, D_out)
    self.linear2 = torch.nn.Linear(D_in, D_out)

  def forward(self, x):
    h1 = self.linear1(x)
    h2 = self.linear2(x)
    return (h1 * h2).clamp(min=0)

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

model = torch.nn.Sequential(
            ParallelBlock(D_i, H),
            ParallelBlock(H, H),
            torch.nn.Linear(H, D_out))
```

---

## 9.9 PyTorch: DataLoaders

PyTorchì—ì„œëŠ” ë°ì´í„°ì…‹ì„ ë‹¤ë£¨ê¸° ì‰½ë„ë¡ `DataLoader` í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤.

- `datasets`: samples, labelsë¥¼ ë‹´ê³  ìˆë‹¤.

- `DataLoader`: datasetì„ iterableí•˜ë„ë¡ wrappingí•œë‹¤.

   minibatching, shuffling, sampling, multithreading ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

- iterationë§ˆë‹¤ modelì€ ë” ë‚˜ì€ ì˜ˆì¸¡ì„ í•˜ë„ë¡ í•™ìŠµëœë‹¤.

  ![iteration](images/iteration.png)

```Python
import torch
from torch.utils.data import TensorDataset, DataLoader

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# (x,y)ë¥¼ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¬¶ì–´ì¤€ë‹¤.
# batch_size=8ë¡œ ì„¤ì •í•˜ì—¬, minibatchë¡œ ë°ì´í„°ë¥¼ 8ê°œì”© ë¬¶ì–´ì¤€ë‹¤.
loader = DataLoader(TensorDataset(x,y), batch_size=8)
model = TwoLayerNet(D_in, H, D_out)

optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)

# 20 epoch
for epoch in range(20):
  # minibatchë§ˆë‹¤ forward, backward, updateë¥¼ ìˆ˜í–‰í•œë‹¤.
  for x_batch, y_batch in loader:
    y_pred = model(x_batch)
    loss = torch.nn.functional.mse_loss(y_pred, y_batch)

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

---

## 9.10 PyTorch: Saving and Loading Models

- `torch.save()` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•œë‹¤.

- `state_dict`: ê° layerì™€ parameter tensorë¥¼ ë§¤í•‘í•´ ë‘” dictionary

```Python
torch.save(model.state_dict(), PATH)
```

- `torch.load()` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

```Python
model = NeuralNetwork()
model.load_state_dict(torch.load(PATH))
```

---

## 9.11 PyTorch: Pretrained Models

torchvision ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ pretrained modelì„ ì‰½ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

> [PyTorch Docs: MODELS AND PRE-TRAINED WEIGHTS](https://pytorch.org/vision/stable/models.html)

> [PyTorch github: pytorch/vision](https://github.com/pytorch/vision)

- quantization(ì–‘ìí™”)ì„ ê±°ì¹œ ëª¨ë¸ë„ ì œê³µí•œë‹¤.

- `weights` ì˜µì…˜ì„ ì´ìš©í•˜ë©´ ë°ì´í„°ì…‹ì„ íŠ¹ì •í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.

```Python
import torch
import torchvision

# AlexNet
alexnet = torchvision.models.alexnet(pretrained=True)

# VGG-16
vgg16 = torchvision.models.vgg16(pretrained=True)

# ResNet-101
resnet101 = torchvision.models.resnet101(pretrained=True)
```

---

## 9.12 PyTorch: Dynamic Computation Graphs

ê·¸ëŸ¬ë‚˜ ì•ì„œ backwardê°€ ëë‚˜ë©´ graphëŠ” destroyë˜ì—ˆë‹¤. ë”°ë¼ì„œ ì´ ë§ì€ iterationë§ˆë‹¤ ë§¤ë²ˆ graphë¥¼ ì¬êµ¬ì¶•í•˜ëŠ” ë¹„íš¨ìœ¨ì ì¸ ê³¼ì •ì„ ê±°ì³ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ êµ¬í˜„ì´ ê°–ëŠ” ì¥ì ì´ ìˆëŠ”ë°, Pythonì˜ control flowë¥¼ ê·¸ëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

íŠ¹íˆ TensorFlowì™€ PyTorchì˜ ê°€ì¥ í° ì°¨ì´ì ìœ¼ë¡œ ê¼½íˆëŠ” ë¶€ë¶„ì´ê¸°ë„ í•˜ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì´ ë‘˜ì„ ë¹„êµí•œ ê·¸ë¦¼ì´ë‹¤.

![static vs dynamic](images/static_vs_dynamic.png)

- static graphëŠ” ë™ì‘ ì „ ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì¶”ê°€ì ì¸ ì¥ì ì„ ê°–ëŠ”ë‹¤.

  ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ operationì„ í•©ì¹˜ëŠ” ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

  ![static graph optimization ex](images/static_graph_optimization.png)

- ë˜í•œ static graphëŠ” **serialize**ê°€ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì—, C++ê³¼ ê°™ì€ ì–¸ì–´ë¡œ deployí•´ì„œ ë¹ ë¥´ê²Œ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë‹¤.

---

### 9.11.1 Static Graph with JIT

í•˜ì§€ë§Œ PyTorchì—ì„œë„ JIT(Just In Time) ì»´íŒŒì¼ëŸ¬ë¥¼ ì‚¬ìš©í•˜ë©´ static graphë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

- ì‹ ê²½ë§ ëª¨ë¸ì„ Python í•¨ìˆ˜ë¡œ êµ¬í˜„í•œë‹¤.

  > ì˜ˆì‹œë¥¼ ìœ„í•´ ì–´ë–¤ weight matrixë¥¼ ì‚¬ìš©í• ì§€ ì •í•˜ëŠ” w2a, w2b, prev_lossë¥¼ ì¶”ê°€í•˜ì˜€ì§€ë§Œ, ì‹¤ì œë¡œ ì´ëŸ¬í•œ êµ¬í˜„ì„ í†µí•´ static graphë¥¼ êµ¬ì„±í•´ì„œëŠ” ì•ˆ ëœë‹¤.

  ![static graph ex](images/static_graph_ex.png)

- ì •ì˜í•œ ì‹ ê²½ë§ ëª¨ë¸ í•¨ìˆ˜ ìœ„ì— `@torch.jit.script` annotationì„ ë¶™ì¸ë‹¤.

```Python
import torch

@torch.jit.script
def model(x, y, w1, w2a, w2b, prev_loss):
  w2 = w2a if prev_loss < 5.0 else w2b
  y_pred = x.mm(w1).clamp(min=0).mm(w2)
  loss = (y_pred - y).pow(2).sum()
  return loss

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)
w1 = torch.randn(D_in, H, requires_grad=True)
w2a = torch.randn(H, D_out, requires_grad=True)
w2b = torch.randn(H, D_out, requires_grad=True)

prev_loss = 5.0
learning_rate = 1e-6
for t in range(500):
  # ì•ì„œ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œë‹¤.
  loss = model(x, y, w1, w2a, w2b, prev_loss)

  loss.backward()
  prev_loss = loss.item()
```

---